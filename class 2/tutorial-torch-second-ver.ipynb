{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We download the data in the same way we did when using ````sklearn```` library.\n",
    "\n",
    "Note that here we also chagne the **data type** to ````torch.tensor````, because we want to use functions from the ````pytorch```` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "digits = datasets.load_digits()\n",
    "img = digits.images\n",
    "labels = digits.target\n",
    "\n",
    "X = img.reshape((len(img),-1))\n",
    "from sklearn.model_selection import train_test_split\n",
    "test_size=0.2\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, labels, test_size = test_size, shuffle=False)\n",
    "\n",
    "X_train = X_train.astype(np.float32)\n",
    "X_train = torch.tensor(X_train)\n",
    "X_test = torch.tensor(X_test)\n",
    "y_train = torch.tensor(y_train)\n",
    "y_train=y_train.long()\n",
    "MLPClassifiery_test = torch.tensor(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1 \n",
    "\n",
    "Let's build a ````Network```` class. This will be your model.\n",
    "\n",
    "You will need\n",
    "* ````__init__```` function defining the attributes. These attributes should be the different layers of your model.\n",
    "* You can define a linear (fully-connected) layer using ````Linear(input_size,output_size)```` function from \"torch.nn\" library (imported as \"nn\")\n",
    "* You can define an activation function using ````ReLU()```` function, also from \"torch.nn\" library\n",
    "* ````forward```` function, defining how the input is processed in the forward pass, layer by layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Network(nn.Module):\n",
    "    def __init__(self,input_size,output_size):\n",
    "        super(Network, self).__init__()\n",
    "        self.linear_layer=nn.Linear(input_size,output_size)\n",
    "        self.relu=nn.ReLU()\n",
    "    def forward(self,x):\n",
    "        out=self.linear_layer(x)\n",
    "        out=self.relu(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2\n",
    "Define your model as instance of your Network class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=Network(64,32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we define the optimized and loss function.\n",
    "\n",
    "*For now you can just use this as it is. As you learn more about Machine Learning, you might want to experiment with changing different parameters in this setup, or even picking a different loss function and a better optimizer.*\n",
    "\n",
    "*It takes a lot of experience to learn all bits and pieces of this machinery, and all programmers spend a long time just copy+paste-ing this default setup to their code.*\n",
    "\n",
    "*The rule of thumb is: if it works well, let's use it and see what happens!*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate =0.001\n",
    "epochs = 200\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)\n",
    "loss_func = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q3\n",
    "Model training.\n",
    "\n",
    "You will train your model in a loop. Loop should be over epochs (\"training epochs\"). We will do this step by step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def how_loss_changes_as_epoch_changes(epoch):\n",
    "    storage_of_loss=[]\n",
    "    for e in range(epoch):\n",
    "\n",
    "        ## first, at the beginning of the loop, make sure that the optimizer kills all previous grad values to avoid using them again in your new iteration\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # now, perform the forward step by applying your model to your dataset\n",
    "        output = model.forward(X_train)\n",
    "\n",
    "        # calculate the loss, using the \"loss_func\" applied to the output from your model compared with the real labels\n",
    "        loss =loss_func(output,y_train)\n",
    "        l='{:.6f}'.format(loss)\n",
    "        m=float(l)\n",
    "        storage_of_loss.append(m)\n",
    "        # MAGIC time: perform the back-prop:\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # at each 10 steps, let's print the value of the loss to see if it is changing.\n",
    "        if e%10==0:\n",
    "            print('Epoch: {} - Loss: {:.6f}'.format(e, loss))\n",
    "    print(\"all the loss are saved\",storage_of_loss)\n",
    "    return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 - Loss: 8.152740\n",
      "Epoch: 10 - Loss: 3.893719\n",
      "Epoch: 20 - Loss: 1.970841\n",
      "Epoch: 30 - Loss: 1.375838\n",
      "Epoch: 40 - Loss: 1.000165\n",
      "Epoch: 50 - Loss: 0.603992\n",
      "Epoch: 60 - Loss: 0.367051\n",
      "Epoch: 70 - Loss: 0.258301\n",
      "Epoch: 80 - Loss: 0.204968\n",
      "Epoch: 90 - Loss: 0.176085\n",
      "Epoch: 100 - Loss: 0.158929\n",
      "Epoch: 110 - Loss: 0.147239\n",
      "Epoch: 120 - Loss: 0.138468\n",
      "Epoch: 130 - Loss: 0.131443\n",
      "Epoch: 140 - Loss: 0.125570\n",
      "Epoch: 150 - Loss: 0.120508\n",
      "Epoch: 160 - Loss: 0.116051\n",
      "Epoch: 170 - Loss: 0.112065\n",
      "Epoch: 180 - Loss: 0.108460\n",
      "Epoch: 190 - Loss: 0.105168\n",
      "all the loss are saved [8.15274, 7.718696, 6.994428, 6.156381, 5.409637, 4.926834, 4.690975, 4.525872, 4.339681, 4.130936, 3.893719, 3.591915, 3.243613, 2.97013, 2.846514, 2.797245, 2.718455, 2.568877, 2.362587, 2.145787, 1.970841, 1.864849, 1.815883, 1.791278, 1.759325, 1.702216, 1.621982, 1.536011, 1.463062, 1.410712, 1.375838, 1.349532, 1.322788, 1.292095, 1.255291, 1.212901, 1.169037, 1.12322, 1.077273, 1.035271, 1.000165, 0.973957, 0.950731, 0.920892, 0.879116, 0.828862, 0.778535, 0.732181, 0.686856, 0.643412, 0.603992, 0.570588, 0.542298, 0.517428, 0.49266, 0.466968, 0.441372, 0.418043, 0.398227, 0.381632, 0.367051, 0.353298, 0.339776, 0.326472, 0.313582, 0.301554, 0.290784, 0.281313, 0.272956, 0.265403, 0.258301, 0.251365, 0.24467, 0.238261, 0.232156, 0.226531, 0.221432, 0.216821, 0.212611, 0.208692, 0.204968, 0.201379, 0.197904, 0.194554, 0.19136, 0.18835, 0.185539, 0.182929, 0.180501, 0.178229, 0.176085, 0.174044, 0.172086, 0.170202, 0.168387, 0.166641, 0.164964, 0.163356, 0.161816, 0.160342, 0.158929, 0.157573, 0.156268, 0.155011, 0.153796, 0.15262, 0.151481, 0.150374, 0.149299, 0.148254, 0.147239, 0.146252, 0.145293, 0.144361, 0.143455, 0.142573, 0.141713, 0.140875, 0.140055, 0.139253, 0.138468, 0.137699, 0.136946, 0.136209, 0.135486, 0.134779, 0.134086, 0.133406, 0.13274, 0.132086, 0.131443, 0.130812, 0.130191, 0.12958, 0.128979, 0.128388, 0.127806, 0.127234, 0.12667, 0.126116, 0.12557, 0.125032, 0.124501, 0.123978, 0.123462, 0.122953, 0.122451, 0.121956, 0.121467, 0.120984, 0.120508, 0.120038, 0.119573, 0.119114, 0.118661, 0.118214, 0.117771, 0.117334, 0.116901, 0.116474, 0.116051, 0.115633, 0.11522, 0.114811, 0.114406, 0.114006, 0.11361, 0.113218, 0.11283, 0.112446, 0.112065, 0.111689, 0.111316, 0.110947, 0.110582, 0.11022, 0.109861, 0.109506, 0.109154, 0.108805, 0.10846, 0.108117, 0.107778, 0.107442, 0.107108, 0.106778, 0.106451, 0.106126, 0.105804, 0.105485, 0.105168, 0.104855, 0.104543, 0.104235, 0.103928, 0.103625, 0.103324, 0.103025, 0.102728, 0.102434]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.102434"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "how_loss_changes_as_epoch_changes(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "only integer scalar arrays can be converted to a scalar index",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-47-478077ad3463>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinspace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m200\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m201\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mlosses\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mhow_loss_changes_as_epoch_changes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlosses\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-45-77ca3db7b138>\u001b[0m in \u001b[0;36mhow_loss_changes_as_epoch_changes\u001b[1;34m(epoch)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mhow_loss_changes_as_epoch_changes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mstorage_of_loss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[1;31m## first, at the beginning of the loop, make sure that the optimizer kills all previous grad values to avoid using them again in your new iteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: only integer scalar arrays can be converted to a scalar index"
     ]
    }
   ],
   "source": [
    "e=np.linspace(0,200,201)\n",
    "losses=how_loss_changes_as_epoch_changes(e)\n",
    "plt.plot(e,losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q4\n",
    "Modify your code so that\n",
    "* It only prints the value of the loss every 10 steps\n",
    "* saves down values of loss and values of accuracy at each epoch (each step)\n",
    "\n",
    "Then, plot on separate graphs \n",
    "* the accuracy as function of epochs\n",
    "* the loss as function of epochs\n",
    "\n",
    "Is the model training well?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
